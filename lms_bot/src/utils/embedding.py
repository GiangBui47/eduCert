from langchain_community.embeddings import HuggingFaceEmbeddings
from typing import Optional
import torch
import sys

def get_embedding_model(model_name: Optional[str] = None) -> HuggingFaceEmbeddings:
    model_name = model_name or "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    device = "cuda" if torch.cuda.is_available() else "cpu"

    try:
        print(f"üîÑ ƒêang t·∫£i model: {model_name} tr√™n {device.upper()} ...")

        # ‚úÖ ƒê√∫ng c√∫ ph√°p c·ªßa LangChain
        model = HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs={"device": device},  # quan tr·ªçng
            encode_kwargs={"normalize_embeddings": True}
        )

        print(f"‚úÖ Loaded embedding model: {model_name} on {device.upper()}")
        return model

    except Exception as e:
        print(f"‚ùå Kh√¥ng th·ªÉ t·∫£i model '{model_name}' tr√™n {device.upper()}: {e}", file=sys.stderr)

        # Fallback sang CPU n·∫øu GPU b·ªã l·ªói
        if device == "cuda":
            try:
                print("‚ö†Ô∏è Th·ª≠ l·∫°i tr√™n CPU ...")
                model = HuggingFaceEmbeddings(
                    model_name=model_name,
                    model_kwargs={"device": "cpu"},
                    encode_kwargs={"normalize_embeddings": True}
                )
                print(f"‚úÖ Loaded embedding model: {model_name} on CPU (fallback)")
                return model
            except Exception as e2:
                raise RuntimeError(f"L·ªói khi t·∫£i model {model_name} (CPU fallback): {e2}") from e2
        else:
            raise RuntimeError(f"L·ªói khi t·∫£i model {model_name}: {e}") from e
